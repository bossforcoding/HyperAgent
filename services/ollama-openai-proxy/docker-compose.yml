version: '3.8'

services:
  ollama-proxy:
    build: .
    container_name: ollama-openai-proxy
    ports:
      - "5000:80"  # Map host port 5000 to container port 80
    environment:
      # Configure the Ollama server URL
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      # Configure request timeout (in seconds)
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-300.0}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - ollama-network

networks:
  ollama-network:
    driver: bridge